{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9275585e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters(trainable) 7762465(7762465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_number :  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\syed_fahim_ahmed\\anaconda3\\envs\\topo_loss\\lib\\site-packages\\ipykernel_launcher.py:508: DeprecationWarning: Call to deprecated class PersImage. (Replaced with the class `persim.PersistenceImager`.) -- Deprecated since version 0.1.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_number :  2\n",
      "batch_number :  3\n",
      "batch_number :  4\n",
      "batch_number :  5\n",
      "batch_number :  6\n",
      "batch_number :  7\n",
      "batch_number :  8\n",
      "batch_number :  9\n",
      "batch_number :  10\n",
      "batch_number :  11\n",
      "batch_number :  12\n",
      "batch_number :  13\n",
      "batch_number :  14\n",
      "batch_number :  15\n",
      "batch_number :  16\n",
      "batch_number :  17\n",
      "batch_number :  18\n",
      "batch_number :  19\n",
      "batch_number :  20\n",
      "batch_number :  21\n",
      "batch_number :  22\n",
      "batch_number :  23\n",
      "batch_number :  24\n",
      "batch_number :  25\n",
      "batch_number :  26\n",
      "batch_number :  27\n",
      "batch_number :  28\n",
      "batch_number :  29\n",
      "batch_number :  30\n",
      "batch_number :  31\n",
      "batch_number :  32\n",
      "batch_number :  33\n",
      "batch_number :  34\n",
      "batch_number :  35\n",
      "batch_number :  36\n",
      "batch_number :  37\n",
      "batch_number :  38\n",
      "batch_number :  39\n",
      "batch_number :  40\n",
      "batch_number :  41\n",
      "batch_number :  42\n",
      "batch_number :  43\n",
      "batch_number :  44\n",
      "batch_number :  45\n",
      "batch_number :  46\n",
      "batch_number :  47\n",
      "batch_number :  48\n",
      "batch_number :  49\n",
      "batch_number :  50\n",
      "batch_number :  51\n",
      "batch_number :  52\n",
      "batch_number :  53\n",
      "batch_number :  54\n",
      "batch_number :  55\n",
      "batch_number :  56\n",
      "batch_number :  57\n",
      "batch_number :  58\n",
      "batch_number :  59\n",
      "batch_number :  60\n",
      "batch_number :  61\n",
      "batch_number :  62\n",
      "batch_number :  63\n",
      "batch_number :  64\n",
      "batch_number :  65\n",
      "batch_number :  66\n",
      "batch_number :  67\n",
      "batch_number :  68\n",
      "batch_number :  69\n",
      "batch_number :  70\n",
      "batch_number :  71\n",
      "batch_number :  72\n",
      "batch_number :  73\n",
      "batch_number :  74\n",
      "batch_number :  75\n",
      "batch_number :  76\n",
      "batch_number :  77\n",
      "batch_number :  78\n",
      "batch_number :  79\n",
      "batch_number :  80\n",
      "batch_number :  81\n",
      "batch_number :  82\n",
      "batch_number :  83\n",
      "batch_number :  84\n",
      "batch_number :  85\n",
      "batch_number :  86\n",
      "batch_number :  87\n",
      "batch_number :  88\n",
      "batch_number :  89\n",
      "batch_number :  90\n",
      "batch_number :  91\n",
      "batch_number :  92\n",
      "batch_number :  93\n",
      "batch_number :  94\n",
      "batch_number :  95\n",
      "batch_number :  96\n",
      "batch_number :  97\n",
      "batch_number :  98\n",
      "batch_number :  99\n",
      "batch_number :  100\n",
      "batch_number :  101\n",
      "batch_number :  102\n",
      "batch_number :  103\n",
      "batch_number :  104\n",
      "batch_number :  105\n",
      "batch_number :  106\n",
      "batch_number :  107\n",
      "batch_number :  108\n",
      "batch_number :  109\n",
      "batch_number :  110\n",
      "batch_number :  111\n",
      "batch_number :  112\n",
      "batch_number :  113\n",
      "batch_number :  114\n",
      "batch_number :  115\n",
      "batch_number :  116\n",
      "batch_number :  117\n",
      "batch_number :  118\n",
      "batch_number :  119\n",
      "batch_number :  120\n",
      "batch_number :  121\n",
      "batch_number :  122\n",
      "batch_number :  123\n",
      "batch_number :  124\n",
      "batch_number :  125\n",
      "batch_number :  126\n",
      "batch_number :  127\n",
      "batch_number :  128\n",
      "batch_number :  129\n",
      "batch_number :  130\n",
      "batch_number :  131\n",
      "batch_number :  132\n",
      "batch_number :  133\n",
      "batch_number :  134\n",
      "batch_number :  135\n",
      "batch_number :  136\n",
      "batch_number :  137\n",
      "batch_number :  138\n",
      "batch_number :  139\n",
      "batch_number :  140\n",
      "batch_number :  141\n",
      "batch_number :  142\n",
      "batch_number :  143\n",
      "batch_number :  144\n",
      "batch_number :  145\n",
      "batch_number :  146\n",
      "batch_number :  147\n",
      "batch_number :  148\n",
      "batch_number :  149\n",
      "batch_number :  150\n",
      "batch_number :  151\n",
      "batch_number :  152\n",
      "batch_number :  153\n",
      "batch_number :  154\n",
      "batch_number :  155\n",
      "batch_number :  156\n",
      "batch_number :  157\n",
      "batch_number :  158\n",
      "batch_number :  159\n",
      "batch_number :  160\n",
      "batch_number :  161\n",
      "batch_number :  162\n",
      "batch_number :  163\n",
      "batch_number :  164\n",
      "batch_number :  165\n",
      "batch_number :  166\n",
      "batch_number :  167\n",
      "batch_number :  168\n",
      "batch_number :  169\n",
      "batch_number :  170\n",
      "batch_number :  171\n",
      "batch_number :  172\n",
      "batch_number :  173\n",
      "batch_number :  174\n",
      "batch_number :  175\n",
      "batch_number :  176\n",
      "batch_number :  177\n",
      "batch_number :  178\n",
      "batch_number :  179\n",
      "batch_number :  180\n",
      "batch_number :  181\n",
      "batch_number :  182\n",
      "batch_number :  183\n",
      "batch_number :  184\n",
      "batch_number :  185\n",
      "batch_number :  186\n",
      "batch_number :  187\n",
      "batch_number :  188\n",
      "batch_number :  189\n",
      "batch_number :  190\n",
      "batch_number :  191\n",
      "batch_number :  192\n",
      "batch_number :  193\n",
      "batch_number :  194\n",
      "batch_number :  195\n",
      "batch_number :  196\n",
      "batch_number :  197\n",
      "batch_number :  198\n",
      "batch_number :  199\n",
      "batch_number :  200\n",
      "batch_number :  201\n",
      "batch_number :  202\n",
      "batch_number :  203\n",
      "batch_number :  204\n",
      "batch_number :  205\n",
      "batch_number :  206\n",
      "batch_number :  207\n",
      "batch_number :  208\n",
      "batch_number :  209\n",
      "batch_number :  210\n",
      "batch_number :  211\n",
      "batch_number :  212\n",
      "batch_number :  213\n",
      "batch_number :  214\n",
      "batch_number :  215\n",
      "batch_number :  216\n",
      "batch_number :  217\n",
      "batch_number :  218\n",
      "batch_number :  219\n",
      "batch_number :  220\n",
      "batch_number :  221\n",
      "batch_number :  222\n",
      "batch_number :  223\n",
      "batch_number :  224\n",
      "batch_number :  225\n",
      "batch_number :  226\n",
      "batch_number :  227\n",
      "batch_number :  228\n",
      "batch_number :  229\n",
      "batch_number :  230\n",
      "batch_number :  231\n",
      "batch_number :  232\n",
      "batch_number :  233\n",
      "batch_number :  234\n",
      "batch_number :  235\n",
      "batch_number :  236\n",
      "batch_number :  237\n",
      "batch_number :  238\n",
      "batch_number :  239\n",
      "batch_number :  240\n",
      "batch_number :  241\n",
      "batch_number :  242\n",
      "batch_number :  243\n",
      "batch_number :  244\n",
      "batch_number :  245\n",
      "batch_number :  246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:08<13:37,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_number :  1\n",
      "batch_number :  2\n",
      "batch_number :  3\n",
      "batch_number :  4\n",
      "batch_number :  5\n",
      "batch_number :  6\n",
      "batch_number :  7\n",
      "batch_number :  8\n",
      "batch_number :  9\n",
      "batch_number :  10\n",
      "batch_number :  11\n",
      "batch_number :  12\n",
      "batch_number :  13\n",
      "batch_number :  14\n",
      "batch_number :  15\n",
      "batch_number :  16\n",
      "batch_number :  17\n",
      "batch_number :  18\n",
      "batch_number :  19\n",
      "batch_number :  20\n",
      "batch_number :  21\n",
      "batch_number :  22\n",
      "batch_number :  23\n",
      "batch_number :  24\n",
      "batch_number :  25\n",
      "batch_number :  26\n",
      "batch_number :  27\n",
      "batch_number :  28\n",
      "batch_number :  29\n",
      "batch_number :  30\n",
      "batch_number :  31\n",
      "batch_number :  32\n",
      "batch_number :  33\n",
      "batch_number :  34\n",
      "batch_number :  35\n",
      "batch_number :  36\n",
      "batch_number :  37\n",
      "batch_number :  38\n",
      "batch_number :  39\n",
      "batch_number :  40\n",
      "batch_number :  41\n",
      "batch_number :  42\n",
      "batch_number :  43\n",
      "batch_number :  44\n",
      "batch_number :  45\n",
      "batch_number :  46\n",
      "batch_number :  47\n",
      "batch_number :  48\n",
      "batch_number :  49\n",
      "batch_number :  50\n",
      "batch_number :  51\n",
      "batch_number :  52\n",
      "batch_number :  53\n",
      "batch_number :  54\n",
      "batch_number :  55\n",
      "batch_number :  56\n",
      "batch_number :  57\n",
      "batch_number :  58\n",
      "batch_number :  59\n",
      "batch_number :  60\n",
      "batch_number :  61\n",
      "batch_number :  62\n",
      "batch_number :  63\n",
      "batch_number :  64\n",
      "batch_number :  65\n",
      "batch_number :  66\n",
      "batch_number :  67\n",
      "batch_number :  68\n",
      "batch_number :  69\n",
      "batch_number :  70\n",
      "batch_number :  71\n",
      "batch_number :  72\n",
      "batch_number :  73\n",
      "batch_number :  74\n",
      "batch_number :  75\n",
      "batch_number :  76\n",
      "batch_number :  77\n",
      "batch_number :  78\n",
      "batch_number :  79\n",
      "batch_number :  80\n",
      "batch_number :  81\n",
      "batch_number :  82\n",
      "batch_number :  83\n",
      "batch_number :  84\n",
      "batch_number :  85\n",
      "batch_number :  86\n",
      "batch_number :  87\n",
      "batch_number :  88\n",
      "batch_number :  89\n",
      "batch_number :  90\n",
      "batch_number :  91\n",
      "batch_number :  92\n",
      "batch_number :  93\n",
      "batch_number :  94\n",
      "batch_number :  95\n",
      "batch_number :  96\n",
      "batch_number :  97\n",
      "batch_number :  98\n",
      "batch_number :  99\n",
      "batch_number :  100\n",
      "batch_number :  101\n",
      "batch_number :  102\n",
      "batch_number :  103\n",
      "batch_number :  104\n",
      "batch_number :  105\n",
      "batch_number :  106\n",
      "batch_number :  107\n",
      "batch_number :  108\n",
      "batch_number :  109\n",
      "batch_number :  110\n",
      "batch_number :  111\n",
      "batch_number :  112\n",
      "batch_number :  113\n",
      "batch_number :  114\n",
      "batch_number :  115\n",
      "batch_number :  116\n",
      "batch_number :  117\n",
      "batch_number :  118\n",
      "batch_number :  119\n",
      "batch_number :  120\n",
      "batch_number :  121\n",
      "batch_number :  122\n",
      "batch_number :  123\n",
      "batch_number :  124\n",
      "batch_number :  125\n",
      "batch_number :  126\n",
      "batch_number :  127\n",
      "batch_number :  128\n",
      "batch_number :  129\n",
      "batch_number :  130\n",
      "batch_number :  131\n",
      "batch_number :  132\n",
      "batch_number :  133\n",
      "batch_number :  134\n",
      "batch_number :  135\n",
      "batch_number :  136\n",
      "batch_number :  137\n",
      "batch_number :  138\n",
      "batch_number :  139\n",
      "batch_number :  140\n",
      "batch_number :  141\n",
      "batch_number :  142\n",
      "batch_number :  143\n",
      "batch_number :  144\n",
      "batch_number :  145\n",
      "batch_number :  146\n",
      "batch_number :  147\n",
      "batch_number :  148\n",
      "batch_number :  149\n",
      "batch_number :  150\n",
      "batch_number :  151\n",
      "batch_number :  152\n",
      "batch_number :  153\n",
      "batch_number :  154\n",
      "batch_number :  155\n",
      "batch_number :  156\n",
      "batch_number :  157\n",
      "batch_number :  158\n",
      "batch_number :  159\n",
      "batch_number :  160\n",
      "batch_number :  161\n",
      "batch_number :  162\n",
      "batch_number :  163\n",
      "batch_number :  164\n",
      "batch_number :  165\n",
      "batch_number :  166\n",
      "batch_number :  167\n",
      "batch_number :  168\n",
      "batch_number :  169\n",
      "batch_number :  170\n",
      "batch_number :  171\n",
      "batch_number :  172\n",
      "batch_number :  173\n",
      "batch_number :  174\n",
      "batch_number :  175\n",
      "batch_number :  176\n",
      "batch_number :  177\n",
      "batch_number :  178\n",
      "batch_number :  179\n",
      "batch_number :  180\n",
      "batch_number :  181\n",
      "batch_number :  182\n",
      "batch_number :  183\n",
      "batch_number :  184\n",
      "batch_number :  185\n",
      "batch_number :  186\n",
      "batch_number :  187\n",
      "batch_number :  188\n",
      "batch_number :  189\n",
      "batch_number :  190\n",
      "batch_number :  191\n",
      "batch_number :  192\n",
      "batch_number :  193\n",
      "batch_number :  194\n",
      "batch_number :  195\n",
      "batch_number :  196\n",
      "batch_number :  197\n",
      "batch_number :  198\n",
      "batch_number :  199\n",
      "batch_number :  200\n",
      "batch_number :  201\n",
      "batch_number :  202\n",
      "batch_number :  203\n",
      "batch_number :  204\n",
      "batch_number :  205\n",
      "batch_number :  206\n",
      "batch_number :  207\n",
      "batch_number :  208\n",
      "batch_number :  209\n",
      "batch_number :  210\n",
      "batch_number :  211\n",
      "batch_number :  212\n",
      "batch_number :  213\n",
      "batch_number :  214\n",
      "batch_number :  215\n",
      "batch_number :  216\n",
      "batch_number :  217\n",
      "batch_number :  218\n",
      "batch_number :  219\n",
      "batch_number :  220\n",
      "batch_number :  221\n",
      "batch_number :  222\n",
      "batch_number :  223\n",
      "batch_number :  224\n",
      "batch_number :  225\n",
      "batch_number :  226\n",
      "batch_number :  227\n",
      "batch_number :  228\n",
      "batch_number :  229\n",
      "batch_number :  230\n",
      "batch_number :  231\n",
      "batch_number :  232\n",
      "batch_number :  233\n",
      "batch_number :  234\n",
      "batch_number :  235\n",
      "batch_number :  236\n",
      "batch_number :  237\n",
      "batch_number :  238\n",
      "batch_number :  239\n",
      "batch_number :  240\n",
      "batch_number :  241\n",
      "batch_number :  242\n",
      "batch_number :  243\n",
      "batch_number :  244\n",
      "batch_number :  245\n",
      "batch_number :  246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:16<13:44,  8.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_number :  1\n",
      "batch_number :  2\n",
      "batch_number :  3\n",
      "batch_number :  4\n",
      "batch_number :  5\n",
      "batch_number :  6\n",
      "batch_number :  7\n",
      "batch_number :  8\n",
      "batch_number :  9\n",
      "batch_number :  10\n",
      "batch_number :  11\n",
      "batch_number :  12\n",
      "batch_number :  13\n",
      "batch_number :  14\n",
      "batch_number :  15\n",
      "batch_number :  16\n",
      "batch_number :  17\n",
      "batch_number :  18\n",
      "batch_number :  19\n",
      "batch_number :  20\n",
      "batch_number :  21\n",
      "batch_number :  22\n",
      "batch_number :  23\n",
      "batch_number :  24\n",
      "batch_number :  25\n",
      "batch_number :  26\n",
      "batch_number :  27\n",
      "batch_number :  28\n",
      "batch_number :  29\n",
      "batch_number :  30\n",
      "batch_number :  31\n",
      "batch_number :  32\n",
      "batch_number :  33\n",
      "batch_number :  34\n",
      "batch_number :  35\n",
      "batch_number :  36\n",
      "batch_number :  37\n",
      "batch_number :  38\n",
      "batch_number :  39\n",
      "batch_number :  40\n",
      "batch_number :  41\n",
      "batch_number :  42\n",
      "batch_number :  43\n",
      "batch_number :  44\n",
      "batch_number :  45\n",
      "batch_number :  46\n",
      "batch_number :  47\n",
      "batch_number :  48\n",
      "batch_number :  49\n",
      "batch_number :  50\n",
      "batch_number :  51\n",
      "batch_number :  52\n",
      "batch_number :  53\n",
      "batch_number :  54\n",
      "batch_number :  55\n",
      "batch_number :  56\n",
      "batch_number :  57\n",
      "batch_number :  58\n",
      "batch_number :  59\n",
      "batch_number :  60\n",
      "batch_number :  61\n",
      "batch_number :  62\n",
      "batch_number :  63\n",
      "batch_number :  64\n",
      "batch_number :  65\n",
      "batch_number :  66\n",
      "batch_number :  67\n",
      "batch_number :  68\n",
      "batch_number :  69\n",
      "batch_number :  70\n",
      "batch_number :  71\n",
      "batch_number :  72\n",
      "batch_number :  73\n",
      "batch_number :  74\n",
      "batch_number :  75\n",
      "batch_number :  76\n",
      "batch_number :  77\n",
      "batch_number :  78\n",
      "batch_number :  79\n",
      "batch_number :  80\n",
      "batch_number :  81\n",
      "batch_number :  82\n",
      "batch_number :  83\n",
      "batch_number :  84\n",
      "batch_number :  85\n",
      "batch_number :  86\n",
      "batch_number :  87\n",
      "batch_number :  88\n",
      "batch_number :  89\n",
      "batch_number :  90\n",
      "batch_number :  91\n",
      "batch_number :  92\n",
      "batch_number :  93\n",
      "batch_number :  94\n",
      "batch_number :  95\n",
      "batch_number :  96\n",
      "batch_number :  97\n",
      "batch_number :  98\n",
      "batch_number :  99\n",
      "batch_number :  100\n",
      "batch_number :  101\n",
      "batch_number :  102\n",
      "batch_number :  103\n",
      "batch_number :  104\n",
      "batch_number :  105\n",
      "batch_number :  106\n",
      "batch_number :  107\n",
      "batch_number :  108\n",
      "batch_number :  109\n",
      "batch_number :  110\n",
      "batch_number :  111\n",
      "batch_number :  112\n",
      "batch_number :  113\n",
      "batch_number :  114\n",
      "batch_number :  115\n",
      "batch_number :  116\n",
      "batch_number :  117\n",
      "batch_number :  118\n",
      "batch_number :  119\n",
      "batch_number :  120\n",
      "batch_number :  121\n",
      "batch_number :  122\n",
      "batch_number :  123\n",
      "batch_number :  124\n",
      "batch_number :  125\n",
      "batch_number :  126\n",
      "batch_number :  127\n",
      "batch_number :  128\n",
      "batch_number :  129\n",
      "batch_number :  130\n",
      "batch_number :  131\n",
      "batch_number :  132\n",
      "batch_number :  133\n",
      "batch_number :  134\n",
      "batch_number :  135\n",
      "batch_number :  136\n",
      "batch_number :  137\n",
      "batch_number :  138\n",
      "batch_number :  139\n",
      "batch_number :  140\n",
      "batch_number :  141\n",
      "batch_number :  142\n",
      "batch_number :  143\n",
      "batch_number :  144\n",
      "batch_number :  145\n",
      "batch_number :  146\n",
      "batch_number :  147\n",
      "batch_number :  148\n",
      "batch_number :  149\n",
      "batch_number :  150\n",
      "batch_number :  151\n",
      "batch_number :  152\n",
      "batch_number :  153\n",
      "batch_number :  154\n",
      "batch_number :  155\n",
      "batch_number :  156\n",
      "batch_number :  157\n",
      "batch_number :  158\n",
      "batch_number :  159\n",
      "batch_number :  160\n",
      "batch_number :  161\n",
      "batch_number :  162\n",
      "batch_number :  163\n",
      "batch_number :  164\n",
      "batch_number :  165\n",
      "batch_number :  166\n",
      "batch_number :  167\n",
      "batch_number :  168\n",
      "batch_number :  169\n",
      "batch_number :  170\n",
      "batch_number :  171\n",
      "batch_number :  172\n",
      "batch_number :  173\n",
      "batch_number :  174\n",
      "batch_number :  175\n",
      "batch_number :  176\n",
      "batch_number :  177\n",
      "batch_number :  178\n",
      "batch_number :  179\n",
      "batch_number :  180\n",
      "batch_number :  181\n",
      "batch_number :  182\n",
      "batch_number :  183\n",
      "batch_number :  184\n",
      "batch_number :  185\n",
      "batch_number :  186\n",
      "batch_number :  187\n",
      "batch_number :  188\n",
      "batch_number :  189\n",
      "batch_number :  190\n",
      "batch_number :  191\n",
      "batch_number :  192\n",
      "batch_number :  193\n",
      "batch_number :  194\n",
      "batch_number :  195\n",
      "batch_number :  196\n",
      "batch_number :  197\n",
      "batch_number :  198\n",
      "batch_number :  199\n",
      "batch_number :  200\n",
      "batch_number :  201\n",
      "batch_number :  202\n",
      "batch_number :  203\n",
      "batch_number :  204\n",
      "batch_number :  205\n",
      "batch_number :  206\n",
      "batch_number :  207\n",
      "batch_number :  208\n",
      "batch_number :  209\n",
      "batch_number :  210\n",
      "batch_number :  211\n",
      "batch_number :  212\n",
      "batch_number :  213\n",
      "batch_number :  214\n",
      "batch_number :  215\n",
      "batch_number :  216\n",
      "batch_number :  217\n",
      "batch_number :  218\n",
      "batch_number :  219\n",
      "batch_number :  220\n",
      "batch_number :  221\n",
      "batch_number :  222\n",
      "batch_number :  223\n",
      "batch_number :  224\n",
      "batch_number :  225\n",
      "batch_number :  226\n",
      "batch_number :  227\n",
      "batch_number :  228\n",
      "batch_number :  229\n",
      "batch_number :  230\n",
      "batch_number :  231\n",
      "batch_number :  232\n",
      "batch_number :  233\n",
      "batch_number :  234\n",
      "batch_number :  235\n",
      "batch_number :  236\n",
      "batch_number :  237\n",
      "batch_number :  238\n",
      "batch_number :  239\n",
      "batch_number :  240\n",
      "batch_number :  241\n",
      "batch_number :  242\n",
      "batch_number :  243\n",
      "batch_number :  244\n",
      "batch_number :  245\n",
      "batch_number :  246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:31<18:08, 11.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_number :  1\n",
      "batch_number :  2\n",
      "batch_number :  3\n",
      "batch_number :  4\n",
      "batch_number :  5\n",
      "batch_number :  6\n",
      "batch_number :  7\n",
      "batch_number :  8\n",
      "batch_number :  9\n",
      "batch_number :  10\n",
      "batch_number :  11\n",
      "batch_number :  12\n",
      "batch_number :  13\n",
      "batch_number :  14\n",
      "batch_number :  15\n",
      "batch_number :  16\n",
      "batch_number :  17\n",
      "batch_number :  18\n",
      "batch_number :  19\n",
      "batch_number :  20\n",
      "batch_number :  21\n",
      "batch_number :  22\n",
      "batch_number :  23\n",
      "batch_number :  24\n",
      "batch_number :  25\n",
      "batch_number :  26\n",
      "batch_number :  27\n",
      "batch_number :  28\n",
      "batch_number :  29\n",
      "batch_number :  30\n",
      "batch_number :  31\n",
      "batch_number :  32\n",
      "batch_number :  33\n",
      "batch_number :  34\n",
      "batch_number :  35\n",
      "batch_number :  36\n",
      "batch_number :  37\n",
      "batch_number :  38\n",
      "batch_number :  39\n",
      "batch_number :  40\n",
      "batch_number :  41\n",
      "batch_number :  42\n",
      "batch_number :  43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:32<17:30, 10.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25900\\4278633761.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25900\\4278633761.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    394\u001b[0m             \u001b[0mlogfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m             \u001b[0mt_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madv_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m             \u001b[0mv_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperformance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m             \u001b[0mnetwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'scheduler'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25900\\4278633761.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(adv_params, network, dataloader, withTopo)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[1;31m#prediction = ((p - torch.min(p))/(torch.max(p) - torch.min(p)))*2-1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25900\\4278633761.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pred, truth)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'mean'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25900\\4278633761.py\u001b[0m in \u001b[0;36m_bce\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1e-7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1e-7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run Archpool.ipynb\n",
    "%run Topo_treatment.ipynb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import model\n",
    "from dataset import *\n",
    "from utils import check_dir\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import sys\n",
    "import json\n",
    "sys.path.insert(0, './persis_lib_cpp')\n",
    "from persis_homo_optimal import *\n",
    "\n",
    "torch_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \n",
    "    '''f = open('example.json')\n",
    "    config = json.load(f)'''\n",
    "    config = return_config()\n",
    "\n",
    "    try:\n",
    "        if config['output_path'][-1] != '/':\n",
    "            config['output_path'] += '/'\n",
    "        if config['train_data_path'][-1] != '/':\n",
    "            config['train_data_path'] += '/'\n",
    "        if config['val_data_path'][-1] != '/':\n",
    "            config['val_data_path'] += '/'\n",
    "    except KeyError as err:\n",
    "        print(f'{opt.config}: Unspecified path {err}')\n",
    "        exit(1)\n",
    "    return config\n",
    "\n",
    "\n",
    "def initialize_network(config):\n",
    "    network = {}\n",
    "    random_seed = 0\n",
    "    try:\n",
    "        config['resume']\n",
    "        random_seed = config['random_seed']\n",
    "    except KeyError:\n",
    "        config['resume'] = False\n",
    "    torch.manual_seed(random_seed)\n",
    "    if (config['resume'] and os.path.isfile(config['resume'])):\n",
    "        confout = config['output_path'] + config['name'] + '/'\n",
    "        network['resume'] = True\n",
    "        checkpoint = torch.load(config['resume'], map_location=torch_device)\n",
    "        network['epoch_start'] = checkpoint['epoch'] + \\\n",
    "            1 if checkpoint['output_dir'] == confout else 0\n",
    "        network['epoch_end'] = config['epoch'] or checkpoint['epoch_end']\n",
    "        network['output_dir'] = confout\n",
    "        network['checkpoint_dir'] = checkpoint['checkpoint_dir']\n",
    "        network['learning_rate'] = checkpoint['learning_rate']\n",
    "        network['train_data_dir'] = checkpoint['train_data_dir']\n",
    "        network['val_data_dir'] = checkpoint['val_data_dir']\n",
    "        network['name'] = checkpoint['name']\n",
    "        network['batch_size'] = checkpoint['batch_size']\n",
    "        network['features'] = checkpoint['features']\n",
    "        network['image_size'] = checkpoint['image_size']\n",
    "        network['image_channels'] = checkpoint['image_channels']\n",
    "        network['optimizer_name'] = checkpoint['optimizer_name']\n",
    "        network['arch'] = checkpoint['arch']\n",
    "        network['bn'] = checkpoint['bn']\n",
    "        network['checkpoint'] = checkpoint\n",
    "    else:\n",
    "        network['resume'] = False\n",
    "        try:\n",
    "            network['output_dir'] = config['output_path'] + \\\n",
    "                config['name'] + '/'\n",
    "            network['name'] = config['name']\n",
    "            network['epoch_start'] = 0\n",
    "            network['epoch_end'] = config['epoch']\n",
    "            network['learning_rate'] = config['learning_rate']\n",
    "            network['batch_size'] = config['batch_size']\n",
    "            network['features'] = int(config['features'])\n",
    "            network['image_size'] = config['image_size']\n",
    "            network['image_channels'] = config['image_channels']\n",
    "            network['optimizer_name'] = config['optimizer']\n",
    "            network['train_data_dir'] = config['train_data_path']\n",
    "            network['val_data_dir'] = config['val_data_path']\n",
    "            network['arch'] = config['arch']\n",
    "            network['bn'] = config['bn']\n",
    "        except KeyError as err:\n",
    "            print(f'Configuration: Unspecified field {err}')\n",
    "            exit(1)\n",
    "    network['checkpoint_dir'] = network['output_dir'] + 'checkpoints/'\n",
    "    network['result_dir'] = network['output_dir'] + 'result/'\n",
    "    check_dir(network['output_dir'])\n",
    "    check_dir(network['checkpoint_dir'])\n",
    "    check_dir(network['result_dir'])\n",
    "\n",
    "    network['logfile_path'] = network['result_dir'] + 'logfile.txt'\n",
    "    network['performance_path'] = network['result_dir'] + 'performance.txt'\n",
    "    learning_model = model.AutoEncoder(\n",
    "        network['image_size'], network['image_channels'], network['features'], network['arch'], network['bn'])\n",
    "    learning_model = learning_model.to(torch_device)\n",
    "    \n",
    "    network['loss_function'] = WeightedBCELoss(one_weight=1,zeros_weight=1)\n",
    "    # network['loss_function'] = nn.MSELoss()\n",
    "    optimizer = None\n",
    "    if network['optimizer_name'] == 'adam':\n",
    "        optimizer = optim.Adam(learning_model.parameters(),\n",
    "                               lr=network['learning_rate'])\n",
    "    elif network['optimizer_name'] == 'sgd':\n",
    "        optimizer = optim.SGD(learning_model.parameters(),\n",
    "                              momentum=0.9, weight_decay=1e-2,\n",
    "                              lr=network['learning_rate'])\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "    if (network['resume']):\n",
    "        learning_model.load_state_dict(network['checkpoint']['model'])\n",
    "        optimizer.load_state_dict(network['checkpoint']['optimizer'])\n",
    "        scheduler.load_state_dict(network['checkpoint']['scheduler'])\n",
    "    \n",
    "    network['model'] = learning_model\n",
    "    network['optimizer'] = optimizer\n",
    "    network['scheduler'] = scheduler\n",
    "    return network\n",
    "\n",
    "\n",
    "class WeightedBCELoss:\n",
    "    def __init__(self, one_weight=1.0, zeros_weight=1.0, reduction=\"mean\"):\n",
    "        self.reduction = reduction\n",
    "        self.update_weights(one_weight, zeros_weight)\n",
    "\n",
    "    def update_weights(self, one_weight, zeros_weight):\n",
    "        self.weights = torch.FloatTensor([one_weight, zeros_weight])\n",
    "        self.weights.to(torch_device)\n",
    "\n",
    "    def _bce(self, x, y):\n",
    "        weights = -self.weights\n",
    "        x = torch.clamp(x, min=1e-7, max=1-1e-7)\n",
    "        y = torch.clamp(y, min=1e-7, max=1-1e-7)\n",
    "        return weights[1]*y*torch.log(x) + weights[0]*(1-y)*torch.log(1-x)\n",
    "\n",
    "    def __call__(self, pred, truth):\n",
    "        loss = self._bce(pred, truth)\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(loss)\n",
    "        if self.reduction == 'sum':\n",
    "            return torch.sum(loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "def train(adv_params, network, dataloader, withTopo=False):\n",
    "    \n",
    "    et = Edges_(adv_params, debug=False)\n",
    "    #criterionT = GANLoss(\"vanilla_topo\", \"sum\").to(torch_device)\n",
    "    criterionT = nn.BCELoss(reduction=\"sum\")\n",
    "    loss_function = network['loss_function']\n",
    "    model = network['model']\n",
    "    optimizer = network['optimizer']\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    t_loss = 0.0\n",
    "    result_dir = network['result_dir']\n",
    "    batch_number = 0\n",
    "    step_num = 0\n",
    "    tot_append_loss = 0.0\n",
    "    for data in dataloader:\n",
    "        \n",
    "        scalars, label = data\n",
    "\n",
    "        scalars = scalars.to(torch_device)\n",
    "        label = label.to(torch_device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        p = model(scalars)\n",
    "        #prediction = ((p - torch.min(p))/(torch.max(p) - torch.min(p)))*2-1\n",
    "        \n",
    "        loss = loss_function(p, label)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        num_rows = p.size(0)\n",
    "        \n",
    "        batch_number += 1\n",
    "        print(\"batch_number : \", batch_number)\n",
    "        \n",
    "        if withTopo:\n",
    "            \n",
    "            tp_wgt   = et.return_tp_weight()\n",
    "            fake_fix, mean_wasdis = et.fix_with_topo(p.detach().cpu().numpy(), label.detach().cpu().numpy(),\n",
    "                                                     et.return_target_dim(), result_dir, batch_number, num_rows, \n",
    "                                                     0.0, 1.0, blind=et.blind())\n",
    "            \n",
    "            fake_fix = torch.from_numpy(fake_fix).to(torch_device)\n",
    "            \n",
    "            fake_fix = torch.unsqueeze(fake_fix, 1)\n",
    "            fake_fix = (fake_fix - torch.min(fake_fix))/(torch.max(fake_fix) - torch.min(fake_fix))\n",
    "            errT = criterionT(p, fake_fix) * tp_wgt\n",
    "            t_loss += errT.item()\n",
    "            tot_loss = loss + errT\n",
    "            tot_append_loss += tot_loss.item()\n",
    "\n",
    "            tot_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    if withTopo:\n",
    "        training_loss = running_loss / len(dataloader.dataset)\n",
    "        topo_loss = t_loss / len(dataloader.dataset)\n",
    "        total_loss = tot_append_loss / len(dataloader.dataset)\n",
    "    \n",
    "        return [training_loss], [topo_loss], [total_loss]\n",
    "    else:\n",
    "        training_loss = running_loss / len(dataloader.dataset)\n",
    "        return [training_loss]\n",
    "\n",
    "\n",
    "def validate(network, dataloader, epoch):\n",
    "    image_size = [network['image_size'], network['image_size']]\n",
    "    running_loss = 0.0\n",
    "    tp = 0.0  # true positive\n",
    "    tn = 0.0  # true negative\n",
    "    fp = 0.0  # false positive\n",
    "    fn = 0.0  # false negative\n",
    "\n",
    "    l1_diff = 0.0\n",
    "    with torch.no_grad():\n",
    "        loss_function = network['loss_function']\n",
    "        model = network['model']\n",
    "        result_dir = network['result_dir']\n",
    "        image_channels = network['image_channels']\n",
    "        model.eval()\n",
    "        batch_number = 0\n",
    "        output_image = False\n",
    "        save_output = False\n",
    "        \n",
    "        if epoch != \"\":\n",
    "            if (epoch != 0) and (epoch != 10) and ((epoch+1)%10 == 0):        \n",
    "                # create directory for the epoch\n",
    "                # add lines\n",
    "                epoch_dir = os.path.join(network['output_dir'], f\"epoch_{epoch}\")\n",
    "                if not os.path.exists(epoch_dir):\n",
    "                    os.makedirs(epoch_dir)\n",
    "                save_output = True\n",
    "        \n",
    "        for i, data in enumerate(dataloader):\n",
    "            scalars, label = data\n",
    "            label = label.to(torch_device)\n",
    "            scalars = scalars.to(torch_device)\n",
    "            batch_size = label.size(0)\n",
    "\n",
    "            prediction = model(scalars)\n",
    "            \n",
    "            loss = loss_function(prediction, label)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            # log accuracy\n",
    "            pred = prediction.cpu().view(batch_size, -1).double()\n",
    "            truth = label.cpu().view(batch_size, -1).double()\n",
    "\n",
    "            plabel = torch.zeros(pred.size())\n",
    "            plabel[pred >= 0.3] = 1\n",
    "            \n",
    "            tp += torch.sum(torch.logical_and(plabel == 1, truth == 1).float())\n",
    "            tn += torch.sum(torch.logical_and(plabel == 0, truth == 0).float())\n",
    "            fp += torch.sum(torch.logical_and(plabel == 1, truth == 0).float())\n",
    "            fn += torch.sum(torch.logical_and(plabel == 0, truth == 1).float())\n",
    "\n",
    "            l1_diff += torch.sum(torch.abs(pred - truth))\n",
    "\n",
    "            if epoch != \"\":\n",
    "                if (epoch == 0) or (epoch == network['epoch_end'] - 1) or (i == len(dataloader) - 1) or (epoch == 3):\n",
    "                    output_image = True\n",
    "\n",
    "            if output_image:\n",
    "                num_rows = batch_size\n",
    "                s = scalars.cpu().view(\n",
    "                    num_rows, 1, image_size[1], image_size[0]).double()\n",
    "                t = label.cpu().view(\n",
    "                    num_rows, 1, image_size[1], image_size[0]).double()\n",
    "\n",
    "                pred = prediction.cpu().view(\n",
    "                    num_rows, 1, image_size[1], image_size[0]).double()\n",
    "                \n",
    "                pl = plabel.cpu().view(\n",
    "                    num_rows, 1, image_size[1], image_size[0]).double()\n",
    "                \n",
    "                out_image = torch.transpose(torch.stack((s, t, pl,pred)), 0, 1).reshape(\n",
    "                    4*num_rows, 1,  image_size[1], image_size[0])\n",
    "                save_image(out_image.cpu(\n",
    "                ), f\"{result_dir}epoch_{epoch}_batch{batch_number}.png\", padding=4, nrow=24)\n",
    "            batch_number += 1\n",
    "            \n",
    "            if save_output:\n",
    "                # save the output as numpy array\n",
    "                # add lines\n",
    "                for j in range(len(prediction)):\n",
    "                    p = np.squeeze(prediction[j].detach().cpu().numpy())\n",
    "                    filename = os.path.join(epoch_dir, f\"output_{i*len(prediction)+j:05}.npy\")\n",
    "                    np.save(filename, p)\n",
    "                    \n",
    "        # end for loop\n",
    "    # end with nograd\n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    l1_diff /= len(dataloader.dataset)\n",
    "    tp /= len(dataloader.dataset)\n",
    "    tn /= len(dataloader.dataset)\n",
    "    fp /= len(dataloader.dataset)\n",
    "    fn /= len(dataloader.dataset)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-9)\n",
    "    precision = (tp)/(tp+fp + 1e-9)\n",
    "    recall = (tp)/(tp+fn + 1e-9)\n",
    "    f1 = 2*tp / (2 * tp + fp + fn + 1e-9)\n",
    "\n",
    "    return [val_loss], [accuracy, precision, recall, f1, l1_diff]\n",
    "\n",
    "\n",
    "def floats2str(l):\n",
    "    return \",\".join(map(lambda x: f'{x:.6f}', l))\n",
    "\n",
    "\n",
    "def parameters_count(model):\n",
    "    total = 0\n",
    "    total_t = 0\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            total += p.numel()\n",
    "            total_t += p.numel()\n",
    "        else:\n",
    "            total += p.numel()\n",
    "    return total, total_t\n",
    "\n",
    "\n",
    "def main():\n",
    "    adv_params  = return_advanced_params()\n",
    "    config = parse_args()\n",
    "    withTopo = False\n",
    "    withTopo = config['withTopo']\n",
    "    network = initialize_network(config)\n",
    "\n",
    "    p, pt = parameters_count(network['model'])\n",
    "    print(f'number of parameters(trainable) {p}({pt})')\n",
    "\n",
    "    with open(network['output_dir']+'config.json', 'w') as jsonout:\n",
    "        json.dump(config, jsonout, indent=2)\n",
    "\n",
    "    train_dataset = ImageBoundary(\n",
    "        config['train_data_path'], network['image_channels'])\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=network['batch_size'], shuffle=True)\n",
    "    val_dataset = ImageBoundary(\n",
    "        config['val_data_path'], network['image_channels'])\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=network['batch_size'], shuffle=False)\n",
    "\n",
    "    '''if network['resume']:\n",
    "        logfile = open(network['logfile_path'], 'a')\n",
    "        perf_log = open(network['performance_path'], 'a')\n",
    "    else:'''\n",
    "    if withTopo:\n",
    "        logfile = open(network['logfile_path'], 'w')\n",
    "        logfile.write('epoch,train_loss,val_loss,topo_loss,total_loss\\n')\n",
    "    else:\n",
    "        logfile = open(network['logfile_path'], 'w')\n",
    "        logfile.write('epoch,train_loss,val_loss\\n')\n",
    "    \n",
    "    perf_log = open(network['performance_path'], 'w')\n",
    "    perf_log.write(\n",
    "        'epoch, accuracy, precision, recall, f1, l1_diff_per_image\\n')\n",
    "\n",
    "    for epoch in tqdm(range(network['epoch_start'], network['epoch_end'])):\n",
    "        if withTopo:\n",
    "            t_loss, topo_loss, total_loss = train(adv_params, network, train_dataloader, True)\n",
    "            v_loss, performance = validate(network, val_dataloader, epoch)\n",
    "            network['scheduler'].step(total_loss[0])\n",
    "            \n",
    "            t_loss = floats2str(t_loss)\n",
    "            v_loss = floats2str(v_loss)\n",
    "            topo_loss = floats2str(topo_loss)\n",
    "            total_loss = floats2str(total_loss)\n",
    "            \n",
    "            logfile.write(f'{epoch},{t_loss},{v_loss},{topo_loss},{total_loss}\\n')\n",
    "            logfile.flush()\n",
    "        else:\n",
    "            t_loss = train(adv_params, network, train_dataloader, False)\n",
    "            v_loss, performance = validate(network, val_dataloader, epoch)\n",
    "            network['scheduler'].step(t_loss[0])\n",
    "            \n",
    "            t_loss = floats2str(t_loss)\n",
    "            v_loss = floats2str(v_loss)\n",
    "            \n",
    "            logfile.write(f'{epoch},{t_loss},{v_loss}\\n')\n",
    "            logfile.flush()\n",
    "        \n",
    "        performance = floats2str(performance)\n",
    "        perf_log.write(f'{epoch},{performance}\\n')\n",
    "        perf_log.flush()\n",
    "        \n",
    "        if ((epoch+1) % 50 == 0) or epoch == network['epoch_end'] - 1:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'epoch_end': network['epoch_end'],\n",
    "                'model': network['model'].state_dict(),\n",
    "                'optimizer': network['optimizer'].state_dict(),\n",
    "                'optimizer_name': network['optimizer_name'],\n",
    "                'scheduler': network['scheduler'].state_dict(),\n",
    "                'checkpoint_dir': network['checkpoint_dir'],\n",
    "                'train_data_dir': network['train_data_dir'],\n",
    "                'val_data_dir': network['val_data_dir'],\n",
    "                'output_dir': network['output_dir'],\n",
    "                'name': network['name'],\n",
    "                'batch_size': network['batch_size'],\n",
    "                'learning_rate': network['learning_rate'],\n",
    "                'features': network['features'],\n",
    "                'image_size': network['image_size'],\n",
    "                'image_channels': network['image_channels'],\n",
    "                'arch': network['arch'],\n",
    "                'bn': network['bn']\n",
    "            }, f'{network[\"checkpoint_dir\"]}{network[\"name\"]}_{epoch}.pth')\n",
    "    logfile.close()\n",
    "    perf_log.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "940172d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute '_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16452\\3282803626.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtorch_device\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/syed_fahim_ahmed/Desktop/Coding_With_Fahim/Unet_MC/TopoSegNetSimple/Output/Unet_Training_BS_10_ep_50/checkpoints/Unet_Training_BS_10_ep_50_49.pth\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\topo_loss\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    787\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\topo_loss\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1131\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\topo_loss\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m             \u001b[0mnbytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m             \u001b[0mload_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute '_utils'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "torch_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "checkpoint = torch.load(\"C:/Users/syed_fahim_ahmed/Desktop/Coding_With_Fahim/Unet_MC/TopoSegNetSimple/Output/Unet_Training_BS_10_ep_50/checkpoints/Unet_Training_BS_10_ep_50_49.pth\", map_location=torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c5f857f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1d3be80b7c8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebea24f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1d3be5feec8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c3f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c427d358109a68739b592cd7c9436d87512ddabfb8edf4c5359ed8348eaf8cbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
