{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9275585e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run Archpool.ipynb\n",
    "%run Topo_treatment.ipynb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import model\n",
    "from dataset import *\n",
    "from utils import check_dir\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "import sys\n",
    "import json\n",
    "sys.path.insert(0, './persis_lib_cpp')\n",
    "from persis_homo_optimal import *\n",
    "\n",
    "torch_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \n",
    "    '''f = open('example.json')\n",
    "    config = json.load(f)'''\n",
    "    config = return_config()\n",
    "\n",
    "    try:\n",
    "        if config['output_path'][-1] != '/':\n",
    "            config['output_path'] += '/'\n",
    "        if config['train_data_path'][-1] != '/':\n",
    "            config['train_data_path'] += '/'\n",
    "        if config['val_data_path'][-1] != '/':\n",
    "            config['val_data_path'] += '/'\n",
    "    except KeyError as err:\n",
    "        print(f'{opt.config}: Unspecified path {err}')\n",
    "        exit(1)\n",
    "    return config\n",
    "\n",
    "\n",
    "def initialize_network(config):\n",
    "    network = {}\n",
    "    random_seed = 0\n",
    "    try:\n",
    "        config['resume']\n",
    "        random_seed = config['random_seed']\n",
    "    except KeyError:\n",
    "        config['resume'] = False\n",
    "    torch.manual_seed(random_seed)\n",
    "    if (config['resume'] and os.path.isfile(config['resume'])):\n",
    "        confout = config['output_path'] + config['name'] + '/'\n",
    "        network['resume'] = True\n",
    "        checkpoint = torch.load(config['resume'], map_location=torch_device)\n",
    "        network['epoch_start'] = checkpoint['epoch'] + \\\n",
    "            1 if checkpoint['output_dir'] == confout else 0\n",
    "        network['epoch_end'] = config['epoch'] or checkpoint['epoch_end']\n",
    "        network['output_dir'] = confout\n",
    "        network['checkpoint_dir'] = checkpoint['checkpoint_dir']\n",
    "        network['learning_rate'] = checkpoint['learning_rate']\n",
    "        network['train_data_dir'] = checkpoint['train_data_dir']\n",
    "        network['val_data_dir'] = checkpoint['val_data_dir']\n",
    "        network['name'] = checkpoint['name']\n",
    "        network['batch_size'] = checkpoint['batch_size']\n",
    "        network['features'] = checkpoint['features']\n",
    "        network['image_size'] = checkpoint['image_size']\n",
    "        network['image_channels'] = checkpoint['image_channels']\n",
    "        network['optimizer_name'] = checkpoint['optimizer_name']\n",
    "        network['downsample_factor'] = checkpoint['downsample_factor']\n",
    "        network['initial_feature_channels'] = checkpoint['initial_feature_channels']\n",
    "        network['arch'] = checkpoint['arch']\n",
    "    else:\n",
    "        network['resume'] = False\n",
    "        try:\n",
    "            network['output_dir'] = config['output_path'] + \\\n",
    "                config['name'] + '/'\n",
    "            network['name'] = config['name']\n",
    "            network['epoch_start'] = 0\n",
    "            network['epoch_end'] = config['epoch']\n",
    "            network['learning_rate'] = config['learning_rate']\n",
    "            network['batch_size'] = config['batch_size']\n",
    "            network['features'] = int(config['features'])\n",
    "            network['image_size'] = config['image_size']\n",
    "            network['image_channels'] = config['image_channels']\n",
    "            network['optimizer_name'] = config['optimizer']\n",
    "            network['train_data_dir'] = config['train_data_path']\n",
    "            network['val_data_dir'] = config['val_data_path']\n",
    "            network['downsample_factor'] = config['downsample_factor']\n",
    "            network['initial_feature_channels'] = config['initial_feature_channels']\n",
    "            network['arch'] = config['arch']\n",
    "        except KeyError as err:\n",
    "            print(f'Configuration: Unspecified field {err}')\n",
    "            exit(1)\n",
    "    network['checkpoint_dir'] = network['output_dir'] + 'checkpoints/'\n",
    "    network['result_dir'] = network['output_dir'] + 'result/'\n",
    "    check_dir(network['output_dir'])\n",
    "    check_dir(network['checkpoint_dir'])\n",
    "    check_dir(network['result_dir'])\n",
    "\n",
    "    network['logfile_path'] = network['result_dir'] + 'logfile.txt'\n",
    "    network['performance_path'] = network['result_dir'] + 'performance.txt'\n",
    "\n",
    "    # image_size, image_channel, features, downsample_factor, initial_feature_channel, arch\n",
    "    learning_model = model.AutoEncoder(network['image_size'],\n",
    "                                       network['image_channels'],\n",
    "                                       network['features'],\n",
    "                                       network['downsample_factor'],\n",
    "                                       network['initial_feature_channels'],\n",
    "                                       network['arch'])\n",
    "    learning_model = learning_model.to(torch_device)\n",
    "\n",
    "    network['loss_function'] = WeightedBCELoss(one_weight=1, zeros_weight=1)\n",
    "    # network['loss_function'] = nn.L1Loss()\n",
    "    optimizer = None\n",
    "    if network['optimizer_name'] == 'adam':\n",
    "        optimizer = optim.Adam(learning_model.parameters(),\n",
    "                               lr=network['learning_rate'])\n",
    "    elif network['optimizer_name'] == 'sgd':\n",
    "        optimizer = optim.SGD(learning_model.parameters(),\n",
    "                              momentum=0.9, weight_decay=1e-2,\n",
    "                              lr=network['learning_rate'])\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "    if (network['resume']):\n",
    "        learning_model.load_state_dict(checkpoint['model'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "\n",
    "    network['model'] = learning_model\n",
    "    network['optimizer'] = optimizer\n",
    "    network['scheduler'] = scheduler\n",
    "    return network\n",
    "\n",
    "\n",
    "class WeightedBCELoss:\n",
    "    def __init__(self, one_weight=1.0, zeros_weight=1.0, reduction=\"mean\"):\n",
    "        self.reduction = reduction\n",
    "        self.update_weights(one_weight, zeros_weight)\n",
    "\n",
    "    def update_weights(self, one_weight, zeros_weight):\n",
    "        self.weights = torch.FloatTensor([one_weight, zeros_weight])\n",
    "        self.weights.to(torch_device)\n",
    "\n",
    "    def _bce(self, x, y):\n",
    "        weights = -self.weights\n",
    "        x = torch.clamp(x, min=1e-7, max=1-1e-7)\n",
    "        y = torch.clamp(y, min=1e-7, max=1-1e-7)\n",
    "        return weights[1]*y*torch.log(x) + weights[0]*(1-y)*torch.log(1-x)\n",
    "\n",
    "    def __call__(self, pred, truth):\n",
    "        loss = self._bce(pred, truth)\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(loss)\n",
    "        if self.reduction == 'sum':\n",
    "            return torch.sum(loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "def train(adv_params, network, dataloader, withTopo=False):\n",
    "    \n",
    "    et = Edges_(adv_params, debug=False)\n",
    "    #criterionT = GANLoss(\"vanilla_topo\", \"sum\").to(torch_device)\n",
    "    criterionT = nn.BCELoss(reduction=\"sum\")\n",
    "    model = network['model']\n",
    "    optimizer = network['optimizer']\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    t_loss = 0.0\n",
    "    result_dir = network['result_dir']\n",
    "    batch_number = 0\n",
    "    step_num = 0\n",
    "    tot_append_loss = 0.0\n",
    "    for data in dataloader:\n",
    "        \n",
    "        scalars, label = data\n",
    "\n",
    "        scalars = scalars.to(torch_device)\n",
    "        label = label.to(torch_device)\n",
    "        batch_size = label.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        p = model(scalars)\n",
    "        #prediction = ((p - torch.min(p))/(torch.max(p) - torch.min(p)))*2-1\n",
    "\n",
    "        zl = torch.sum(label == 0)\n",
    "        ol = torch.sum(label == 1)\n",
    "        weight = zl/ol\n",
    "        loss_function = nn.BCEWithLogitsLoss(pos_weight=weight)\n",
    "        \n",
    "        loss = loss_function(p, label)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        \n",
    "        num_rows = p.size(0)\n",
    "        \n",
    "        batch_number += 1\n",
    "        print(\"batch_number : \", batch_number)\n",
    "        \n",
    "        if withTopo:\n",
    "            sig_layer = nn.Sigmoid()\n",
    "            output = sig_layer(p)\n",
    "            tp_wgt   = et.return_tp_weight()\n",
    "            fake_fix, mean_wasdis = et.fix_with_topo(output.detach().cpu().numpy(), label.detach().cpu().numpy(),\n",
    "                                                     et.return_target_dim(), result_dir, batch_number, num_rows, \n",
    "                                                     0.0, 1.0, blind=et.blind())\n",
    "            \n",
    "            fake_fix = torch.from_numpy(fake_fix).to(torch_device)\n",
    "            \n",
    "            fake_fix = torch.unsqueeze(fake_fix, 1)\n",
    "            fake_fix = (fake_fix - torch.min(fake_fix))/(torch.max(fake_fix) - torch.min(fake_fix))\n",
    "            errT = criterionT(output, fake_fix) * tp_wgt\n",
    "            t_loss += errT.item() * batch_size\n",
    "            tot_loss = loss + errT\n",
    "            tot_append_loss += tot_loss.item() * batch_size\n",
    "\n",
    "            tot_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    if withTopo:\n",
    "        training_loss = running_loss / len(dataloader.dataset)\n",
    "        topo_loss = t_loss / len(dataloader.dataset)\n",
    "        total_loss = tot_append_loss / len(dataloader.dataset)\n",
    "    \n",
    "        return [training_loss], [topo_loss], [total_loss]\n",
    "    else:\n",
    "        training_loss = running_loss / len(dataloader.dataset)\n",
    "        return [training_loss]\n",
    "\n",
    "\n",
    "def validate(network, dataloader, epoch):\n",
    "    image_size = [network['image_size'], network['image_size']]\n",
    "    running_loss = 0.0\n",
    "    tp = 0.0  # true positive\n",
    "    tn = 0.0  # true negative\n",
    "    fp = 0.0  # false positive\n",
    "    fn = 0.0  # false negative\n",
    "    l1_diff = 0.0\n",
    "    with torch.no_grad():\n",
    "        loss_function = network['loss_function']\n",
    "        model = network['model']\n",
    "        result_dir = network['result_dir']\n",
    "        model.eval()\n",
    "        batch_number = 0\n",
    "        output_image = False\n",
    "        save_output = False\n",
    "        \n",
    "        for i, data in enumerate(dataloader):\n",
    "            scalars, label = data\n",
    "            label = label.to(torch_device)\n",
    "            scalars = scalars.to(torch_device)\n",
    "            batch_size = label.size(0)\n",
    "\n",
    "            prediction = model(scalars)\n",
    "\n",
    "            zl = torch.sum(label == 0)\n",
    "            ol = torch.sum(label == 1)\n",
    "            weight = zl/ol\n",
    "            loss_function = nn.BCEWithLogitsLoss(pos_weight=weight)\n",
    "            \n",
    "            loss = loss_function(prediction, label)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            sig_layer = nn.Sigmoid()\n",
    "            output = sig_layer(prediction)\n",
    "            # log accuracy\n",
    "            pred = output.cpu().view(batch_size, -1).double()\n",
    "            truth = label.cpu().view(batch_size, -1).double()\n",
    "\n",
    "            plabel = torch.zeros(pred.size())\n",
    "            plabel[pred >= 0.3] = 1\n",
    "            \n",
    "            tp += torch.sum(torch.logical_and(plabel == 1, truth == 1).float())\n",
    "            tn += torch.sum(torch.logical_and(plabel == 0, truth == 0).float())\n",
    "            fp += torch.sum(torch.logical_and(plabel == 1, truth == 0).float())\n",
    "            fn += torch.sum(torch.logical_and(plabel == 0, truth == 1).float())\n",
    "\n",
    "            l1_diff += torch.sum(torch.abs(pred - truth))\n",
    "\n",
    "            if epoch != \"\":\n",
    "                if (epoch == 0) or (epoch == network['epoch_end'] - 1) or (i == len(dataloader) - 1):\n",
    "                    output_image = True\n",
    "\n",
    "            if output_image:\n",
    "                num_rows = batch_size\n",
    "                s = scalars.cpu().view(\n",
    "                    num_rows, 1, image_size[1], image_size[0]).double()\n",
    "                t = label.cpu().view(\n",
    "                    num_rows, 1, image_size[1], image_size[0]).double()\n",
    "\n",
    "                pred = output.cpu().view(\n",
    "                    num_rows, 1, image_size[1], image_size[0]).double()\n",
    "                \n",
    "                pl = plabel.cpu().view(\n",
    "                    num_rows, 1, image_size[1], image_size[0]).double()\n",
    "                \n",
    "                out_image = torch.transpose(torch.stack((s, t, pl,pred)), 0, 1).reshape(\n",
    "                    4*num_rows, 1,  image_size[1], image_size[0])\n",
    "                save_image(out_image.cpu(\n",
    "                ), f\"{result_dir}epoch_{epoch}_batch{batch_number}.png\", padding=4, nrow=24)\n",
    "            batch_number += 1\n",
    "            \n",
    "            #if save_output:\n",
    "            if epoch != \"\":\n",
    "                if epoch > 10 and batch_number == 1:        \n",
    "                    # create directory for the epoch\n",
    "                    # add lines\n",
    "                    epoch_dir = os.path.join(network['output_dir'], f\"epoch_{epoch}\")\n",
    "                    if not os.path.exists(epoch_dir):\n",
    "                        os.makedirs(epoch_dir)\n",
    "                    save_output = True\n",
    "                    # save the output as numpy array\n",
    "                    # add lines\n",
    "                    for j in range(len(output)):\n",
    "                        p = np.squeeze(output[j].detach().cpu().numpy())\n",
    "                        filename = os.path.join(epoch_dir, f\"output_{i*len(output)+j:05}.npy\")\n",
    "                        np.save(filename, p)\n",
    "                    \n",
    "        # end for loop\n",
    "    # end with nograd\n",
    "    val_loss = running_loss/len(dataloader.dataset)\n",
    "    l1_diff /= len(dataloader.dataset)\n",
    "    tp /= len(dataloader.dataset)\n",
    "    tn /= len(dataloader.dataset)\n",
    "    fp /= len(dataloader.dataset)\n",
    "    fn /= len(dataloader.dataset)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-9)\n",
    "    precision = (tp)/(tp+fp + 1e-9)\n",
    "    recall = (tp)/(tp+fn + 1e-9)\n",
    "    f1 = 2*tp / (2 * tp + fp + fn + 1e-9)\n",
    "\n",
    "    return [val_loss], [accuracy, precision, recall, f1, l1_diff]\n",
    "\n",
    "\n",
    "def floats2str(l):\n",
    "    return \",\".join(map(lambda x: f'{x:.6f}', l))\n",
    "\n",
    "\n",
    "def parameters_count(model):\n",
    "    total = 0\n",
    "    total_t = 0\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            total += p.numel()\n",
    "            total_t += p.numel()\n",
    "        else:\n",
    "            total += p.numel()\n",
    "    return total, total_t\n",
    "\n",
    "\n",
    "def main():\n",
    "    adv_params  = return_advanced_params()\n",
    "    config = parse_args()\n",
    "    withTopo = False\n",
    "    withTopo = config['withTopo']\n",
    "    network = initialize_network(config)\n",
    "\n",
    "    p, pt = parameters_count(network['model'])\n",
    "    print(f'number of parameters(trainable) {p}({pt})')\n",
    "\n",
    "    with open(network['output_dir']+'config.json', 'w') as jsonout:\n",
    "        json.dump(config, jsonout, indent=2)\n",
    "\n",
    "    train_dataset = ImageBoundary(\n",
    "        config['train_data_path'], network['image_channels'])\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=network['batch_size'], shuffle=True)\n",
    "    val_dataset = ImageBoundary(\n",
    "        config['val_data_path'], network['image_channels'])\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset, batch_size=network['batch_size'], shuffle=False)\n",
    "\n",
    "    '''if network['resume']:\n",
    "        logfile = open(network['logfile_path'], 'a')\n",
    "        perf_log = open(network['performance_path'], 'a')\n",
    "    else:'''\n",
    "    if withTopo:\n",
    "        logfile = open(network['logfile_path'], 'w')\n",
    "        logfile.write('epoch,train_loss,val_loss,topo_loss,total_loss\\n')\n",
    "    else:\n",
    "        logfile = open(network['logfile_path'], 'w')\n",
    "        logfile.write('epoch,train_loss,val_loss\\n')\n",
    "    \n",
    "    perf_log = open(network['performance_path'], 'w')\n",
    "    perf_log.write(\n",
    "        'epoch, accuracy, precision, recall, f1, l1_diff_per_image\\n')\n",
    "\n",
    "    for epoch in tqdm(range(network['epoch_start'], network['epoch_end'])):\n",
    "        if withTopo:\n",
    "            t_loss, topo_loss, total_loss = train(adv_params, network, train_dataloader, True)\n",
    "            v_loss, performance = validate(network, val_dataloader, epoch)\n",
    "            network['scheduler'].step(total_loss[0])\n",
    "            \n",
    "            t_loss = floats2str(t_loss)\n",
    "            v_loss = floats2str(v_loss)\n",
    "            topo_loss = floats2str(topo_loss)\n",
    "            total_loss = floats2str(total_loss)\n",
    "            \n",
    "            logfile.write(f'{epoch},{t_loss},{v_loss},{topo_loss},{total_loss}\\n')\n",
    "            logfile.flush()\n",
    "        else:\n",
    "            t_loss = train(adv_params, network, train_dataloader, False)\n",
    "            v_loss, performance = validate(network, val_dataloader, epoch)\n",
    "            network['scheduler'].step(t_loss[0])\n",
    "            \n",
    "            t_loss = floats2str(t_loss)\n",
    "            v_loss = floats2str(v_loss)\n",
    "            \n",
    "            logfile.write(f'{epoch},{t_loss},{v_loss}\\n')\n",
    "            logfile.flush()\n",
    "        \n",
    "        performance = floats2str(performance)\n",
    "        perf_log.write(f'{epoch},{performance}\\n')\n",
    "        perf_log.flush()\n",
    "        \n",
    "        if ((epoch+1) % 50 == 0) or epoch == network['epoch_end'] - 1:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'epoch_end': network['epoch_end'],\n",
    "                'model': network['model'].state_dict(),\n",
    "                'optimizer': network['optimizer'].state_dict(),\n",
    "                'optimizer_name': network['optimizer_name'],\n",
    "                'scheduler': network['scheduler'].state_dict(),\n",
    "                'checkpoint_dir': network['checkpoint_dir'],\n",
    "                'train_data_dir': network['train_data_dir'],\n",
    "                'val_data_dir': network['val_data_dir'],\n",
    "                'output_dir': network['output_dir'],\n",
    "                'name': network['name'],\n",
    "                'batch_size': network['batch_size'],\n",
    "                'learning_rate': network['learning_rate'],\n",
    "                'features': network['features'],\n",
    "                'image_size': network['image_size'],\n",
    "                'image_channels': network['image_channels'],\n",
    "                'downsample_factor': network['downsample_factor'],\n",
    "                'initial_feature_channels': network['initial_feature_channels'],\n",
    "                'arch': network['arch'],\n",
    "            }, f'{network[\"checkpoint_dir\"]}{network[\"name\"]}_{epoch}.pth')\n",
    "    logfile.close()\n",
    "    perf_log.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940172d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "torch_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "checkpoint = torch.load(\"C:/Users/syed_fahim_ahmed/Desktop/Coding_With_Fahim/Unet_MC/TopoSegNetSimple/Output/Unet_Training_BS_10_ep_50/checkpoints/Unet_Training_BS_10_ep_50_49.pth\", map_location=torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5f857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea24f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c3f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c427d358109a68739b592cd7c9436d87512ddabfb8edf4c5359ed8348eaf8cbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
